{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a15019a",
   "metadata": {
    "id": "1rFOZsexuc_2",
    "papermill": {
     "duration": 0.009693,
     "end_time": "2026-02-17T17:00:15.875836",
     "exception": false,
     "start_time": "2026-02-17T17:00:15.866143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Avito Ranking Challenge\n",
    "\n",
    "## Задача\n",
    "Ранжирование объявлений по вероятности контакта пользователя с продавцом.\n",
    "\n",
    "**Метрика**: NDCG@10\n",
    "\n",
    "## Решение\n",
    "\n",
    "**Модель**: CatBoost Ranker с YetiRank loss\n",
    "\n",
    "**Признаки**: 46 features (40 numerical + 6 categorical)\n",
    "- Текстовое сходство (BM25)\n",
    "- Match-индикаторы (location, category, microcategory)\n",
    "- Статистики по query (price, click conversion)\n",
    "- Rank-признаки внутри query\n",
    "- Базовые признаки (price_log, text lengths)\n",
    "- Location match даёт lift 1.49x - самый важный признак\n",
    "\n",
    "\n",
    "## Структура\n",
    "\n",
    "1. **Секции 1-3**: Data Loading + EDA\n",
    "2. **Секции 4-8**: Feature Engineering (BM25 + FeatureExtractor)\n",
    "3. **Секции 9-11**: Model Training (80/20 split, early stopping)\n",
    "4. **Секции 12-14**: Predictions + Submission + Save Model\n",
    "5. **Секция 15**: Model Loading (воспроизводимость без обучения)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Test NDCG = 60%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe64fe27",
   "metadata": {
    "id": "Pl-y19bouc_4",
    "papermill": {
     "duration": 0.009783,
     "end_time": "2026-02-17T17:00:15.893962",
     "exception": false,
     "start_time": "2026-02-17T17:00:15.884179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe5e8e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:00:15.912993Z",
     "iopub.status.busy": "2026-02-17T17:00:15.912655Z",
     "iopub.status.idle": "2026-02-17T17:00:26.687198Z",
     "shell.execute_reply": "2026-02-17T17:00:26.685623Z"
    },
    "id": "igKvlg4fvDH1",
    "outputId": "a9d0162e-0b34-470e-816c-69fdb7c18835",
    "papermill": {
     "duration": 10.787639,
     "end_time": "2026-02-17T17:00:26.690265",
     "exception": false,
     "start_time": "2026-02-17T17:00:15.902626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install catboost --quiet\n",
    "!pip install scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d324910",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:00:26.710628Z",
     "iopub.status.busy": "2026-02-17T17:00:26.710185Z",
     "iopub.status.idle": "2026-02-17T17:00:31.440468Z",
     "shell.execute_reply": "2026-02-17T17:00:31.439488Z"
    },
    "id": "pK_xDH4ouc_4",
    "papermill": {
     "duration": 4.742822,
     "end_time": "2026-02-17T17:00:31.443016",
     "exception": false,
     "start_time": "2026-02-17T17:00:26.700194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import gc\n",
    "import pickle\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "from catboost import CatBoostRanker, Pool\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e00837",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:00:31.461938Z",
     "iopub.status.busy": "2026-02-17T17:00:31.461390Z",
     "iopub.status.idle": "2026-02-17T17:00:31.469223Z",
     "shell.execute_reply": "2026-02-17T17:00:31.468026Z"
    },
    "id": "frWG-Mtduc_5",
    "outputId": "a2531392-777b-41cc-ecf6-0ea44e85ffea",
    "papermill": {
     "duration": 0.020087,
     "end_time": "2026-02-17T17:00:31.471441",
     "exception": false,
     "start_time": "2026-02-17T17:00:31.451354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "# Model Config\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.random_seed = 42\n",
    "        self.iterations = 1000\n",
    "        self.learning_rate = 0.1\n",
    "        self.depth = 6\n",
    "        self.verbose = 50\n",
    "        self.early_stopping_rounds = 50\n",
    "\n",
    "config = Config()\n",
    "np.random.seed(config.random_seed)\n",
    "\n",
    "print(\"Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc8ab9",
   "metadata": {
    "id": "Q52_NPDmuc_5",
    "papermill": {
     "duration": 0.008722,
     "end_time": "2026-02-17T17:00:31.489082",
     "exception": false,
     "start_time": "2026-02-17T17:00:31.480360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aadb276d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:00:31.509230Z",
     "iopub.status.busy": "2026-02-17T17:00:31.508815Z",
     "iopub.status.idle": "2026-02-17T17:01:45.037989Z",
     "shell.execute_reply": "2026-02-17T17:01:45.036611Z"
    },
    "id": "eWPmcqCOuc_6",
    "outputId": "c7d4323a-4fab-436e-c05d-79c031a44453",
    "papermill": {
     "duration": 73.542313,
     "end_time": "2026-02-17T17:01:45.040180",
     "exception": false,
     "start_time": "2026-02-17T17:00:31.497867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (7781790, 14)\n",
      "Test: (335348, 13)\n",
      "Unique queries: 678,190\n",
      "Avg items per query: 11.5\n",
      "Contact rate: 4.41%\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_parquet('/kaggle/input/avito-test/train-dset.parquet')\n",
    "test = pd.read_parquet('/kaggle/input/avito-test/test-dset-small.parquet')\n",
    "\n",
    "print(f\"Train: {train.shape}\")\n",
    "print(f\"Test: {test.shape}\")\n",
    "print(f\"Unique queries: {train['query_id'].nunique():,}\")\n",
    "print(f\"Avg items per query: {train.groupby('query_id').size().mean():.1f}\")\n",
    "print(f\"Contact rate: {train['item_contact'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff06c334",
   "metadata": {
    "id": "uPCA_h7buc_6",
    "papermill": {
     "duration": 0.008462,
     "end_time": "2026-02-17T17:01:45.057403",
     "exception": false,
     "start_time": "2026-02-17T17:01:45.048941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f91d7a",
   "metadata": {
    "id": "bTmHFJ8Buc_6",
    "papermill": {
     "duration": 0.011597,
     "end_time": "2026-02-17T17:01:45.077951",
     "exception": false,
     "start_time": "2026-02-17T17:01:45.066354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.1. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a97b20a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:01:45.096584Z",
     "iopub.status.busy": "2026-02-17T17:01:45.096219Z",
     "iopub.status.idle": "2026-02-17T17:01:45.564508Z",
     "shell.execute_reply": "2026-02-17T17:01:45.563373Z"
    },
    "id": "chbvBCILuc_7",
    "outputId": "d3cce811-eb47-49e1-e068-a114f0fa4be4",
    "papermill": {
     "duration": 0.480242,
     "end_time": "2026-02-17T17:01:45.566728",
     "exception": false,
     "start_time": "2026-02-17T17:01:45.086486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train shape: (7781790, 14)\n",
      "Test shape: (335348, 13)\n",
      "\n",
      "Train queries: 678,190\n",
      "Test queries: 12,505\n",
      "\n",
      "Train items per query:\n",
      "count    678190.000000\n",
      "mean         11.474351\n",
      "std           7.209144\n",
      "min           1.000000\n",
      "25%           7.000000\n",
      "50%          11.000000\n",
      "75%          11.000000\n",
      "max         500.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTrain shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "print(f\"\\nTrain queries: {train['query_id'].nunique():,}\")\n",
    "print(f\"Test queries: {test['query_id'].nunique():,}\")\n",
    "\n",
    "print(f\"\\nTrain items per query:\")\n",
    "print(train.groupby('query_id').size().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49ff53",
   "metadata": {
    "id": "cvUT7j5suc_7",
    "papermill": {
     "duration": 0.008372,
     "end_time": "2026-02-17T17:01:45.583839",
     "exception": false,
     "start_time": "2026-02-17T17:01:45.575467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.2. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32fa8222",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:01:45.602921Z",
     "iopub.status.busy": "2026-02-17T17:01:45.602574Z",
     "iopub.status.idle": "2026-02-17T17:01:48.336343Z",
     "shell.execute_reply": "2026-02-17T17:01:48.334913Z"
    },
    "id": "A2tnQ2Acuc_7",
    "outputId": "0e8598d2-d473-46fa-d2ad-49d98417811d",
    "papermill": {
     "duration": 2.745832,
     "end_time": "2026-02-17T17:01:48.338589",
     "exception": false,
     "start_time": "2026-02-17T17:01:45.592757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in train:\n",
      "  item_title                    :      107 (  0.0%)\n",
      "  item_description              :      107 (  0.0%)\n",
      "  query_mcat                    : 1,761,233 ( 22.6%)\n",
      "\n",
      "Click conv structure:\n",
      "  -1.0 (no data): 6,578,159 (84.5%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing values in train:\")\n",
    "missing = train.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "for col, count in missing.items():\n",
    "    pct = count / len(train) * 100\n",
    "    print(f\"  {col:30s}: {count:>8,} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Click conv structure\n",
    "print(\"\\nClick conv structure:\")\n",
    "missing_conv = (train['item_query_click_conv'] == -1.0).sum()\n",
    "print(f\"  -1.0 (no data): {missing_conv:,} ({missing_conv/len(train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e01a82",
   "metadata": {
    "id": "0Y5Q-wHiuc_8",
    "papermill": {
     "duration": 0.008461,
     "end_time": "2026-02-17T17:01:48.355691",
     "exception": false,
     "start_time": "2026-02-17T17:01:48.347230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.3. Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca8547b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:01:48.374552Z",
     "iopub.status.busy": "2026-02-17T17:01:48.374129Z",
     "iopub.status.idle": "2026-02-17T17:01:48.457668Z",
     "shell.execute_reply": "2026-02-17T17:01:48.456602Z"
    },
    "id": "CLyzRTfcuc_8",
    "outputId": "dcda0a38-df93-4458-ef58-5f8ee3bc9229",
    "papermill": {
     "duration": 0.095672,
     "end_time": "2026-02-17T17:01:48.459904",
     "exception": false,
     "start_time": "2026-02-17T17:01:48.364232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "item_contact distribution:\n",
      "item_contact\n",
      "0.0    7438944\n",
      "1.0     342846\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Overall contact rate: 0.0441 (4.41%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nitem_contact distribution:\")\n",
    "print(train['item_contact'].value_counts().sort_index())\n",
    "\n",
    "contact_rate = train['item_contact'].mean()\n",
    "print(f\"\\nOverall contact rate: {contact_rate:.4f} ({contact_rate*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd1b3ff",
   "metadata": {
    "id": "-A7sb9NZuc_8",
    "papermill": {
     "duration": 0.009551,
     "end_time": "2026-02-17T17:01:48.479095",
     "exception": false,
     "start_time": "2026-02-17T17:01:48.469544",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.4. Price Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bb07d07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:01:48.498309Z",
     "iopub.status.busy": "2026-02-17T17:01:48.497950Z",
     "iopub.status.idle": "2026-02-17T17:01:49.620202Z",
     "shell.execute_reply": "2026-02-17T17:01:49.619105Z"
    },
    "id": "DBLySBgauc_8",
    "outputId": "0f00551c-22d6-42a0-a806-6b8c536e0997",
    "papermill": {
     "duration": 1.134567,
     "end_time": "2026-02-17T17:01:49.622366",
     "exception": false,
     "start_time": "2026-02-17T17:01:48.487799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Price statistics:\n",
      "count    7.781790e+06\n",
      "mean     1.563658e+06\n",
      "std      9.410203e+08\n",
      "min      0.000000e+00\n",
      "25%      6.000000e+02\n",
      "50%      2.600000e+03\n",
      "75%      1.310000e+04\n",
      "max      1.000000e+12\n",
      "Name: price, dtype: float64\n",
      "\n",
      "Zero prices: 655,382 (8.4%)\n",
      "\n",
      "Price percentiles:\n",
      "   25.0%:          600\n",
      "   50.0%:        2,600\n",
      "   75.0%:       13,100\n",
      "   90.0%:       95,000\n",
      "   95.0%:      310,000\n",
      "   99.0%:    4,600,000\n",
      "   99.9%:   22,000,000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPrice statistics:\")\n",
    "print(train['price'].describe())\n",
    "\n",
    "zero_price = (train['price'] == 0).sum()\n",
    "print(f\"\\nZero prices: {zero_price:,} ({zero_price/len(train)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nPrice percentiles:\")\n",
    "for p in [25, 50, 75, 90, 95, 99, 99.9]:\n",
    "    val = train['price'].quantile(p/100)\n",
    "    print(f\"  {p:5.1f}%: {val:>12,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4e2bff",
   "metadata": {
    "id": "Ch9t5X-nuc_8",
    "papermill": {
     "duration": 0.008738,
     "end_time": "2026-02-17T17:01:49.640099",
     "exception": false,
     "start_time": "2026-02-17T17:01:49.631361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.5. Category Match Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc5a8aa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:01:49.660064Z",
     "iopub.status.busy": "2026-02-17T17:01:49.659135Z",
     "iopub.status.idle": "2026-02-17T17:01:55.953205Z",
     "shell.execute_reply": "2026-02-17T17:01:55.952151Z"
    },
    "id": "2uYnvzCTuc_8",
    "outputId": "0f081dea-fd6c-4a5d-b5d1-d926cffaec85",
    "papermill": {
     "duration": 6.306781,
     "end_time": "2026-02-17T17:01:55.955587",
     "exception": false,
     "start_time": "2026-02-17T17:01:49.648806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cat_match:\n",
      "  Match %: 69.8%\n",
      "  Contact rate (match): 0.0456\n",
      "  Contact rate (no match): 0.0404\n",
      "  Lift: 1.13x\n",
      "\n",
      "mcat_match:\n",
      "  Match %: 30.3%\n",
      "  Contact rate (match): 0.0515\n",
      "  Contact rate (no match): 0.0408\n",
      "  Lift: 1.26x\n",
      "\n",
      "loc_match:\n",
      "  Match %: 48.5%\n",
      "  Contact rate (match): 0.0530\n",
      "  Contact rate (no match): 0.0356\n",
      "  Lift: 1.49x\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate lift\n",
    "def calculate_lift(df, match_col, target_col='item_contact'):\n",
    "    \"\"\"Calculate lift for a match feature\"\"\"\n",
    "    match_rate = df[df[match_col] == 1][target_col].mean()\n",
    "    no_match_rate = df[df[match_col] == 0][target_col].mean()\n",
    "    lift = match_rate / no_match_rate if no_match_rate > 0 else 0\n",
    "    return match_rate, no_match_rate, lift\n",
    "\n",
    "# Calculate matches\n",
    "train['cat_match'] = (train['query_cat'] == train['item_cat_id']).astype(int)\n",
    "train['mcat_match'] = (train['query_mcat'] == train['item_mcat_id']).astype(int)\n",
    "train['loc_match'] = (train['query_loc'] == train['item_loc']).astype(int)\n",
    "\n",
    "# Analyze each match type\n",
    "for match_type in ['cat_match', 'mcat_match', 'loc_match']:\n",
    "    match_rate, no_match_rate, lift = calculate_lift(train, match_type)\n",
    "    match_pct = train[match_type].mean() * 100\n",
    "\n",
    "    print(f\"\\n{match_type}:\")\n",
    "    print(f\"  Match %: {match_pct:.1f}%\")\n",
    "    print(f\"  Contact rate (match): {match_rate:.4f}\")\n",
    "    print(f\"  Contact rate (no match): {no_match_rate:.4f}\")\n",
    "    print(f\"  Lift: {lift:.2f}x\")\n",
    "\n",
    "# Clean up temporary columns\n",
    "train.drop(['cat_match', 'mcat_match', 'loc_match'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1690279",
   "metadata": {
    "id": "ZPxuZdWCuc_8",
    "papermill": {
     "duration": 0.009195,
     "end_time": "2026-02-17T17:01:55.974370",
     "exception": false,
     "start_time": "2026-02-17T17:01:55.965175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.6. Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b601908",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:01:55.995578Z",
     "iopub.status.busy": "2026-02-17T17:01:55.995204Z",
     "iopub.status.idle": "2026-02-17T17:02:10.626211Z",
     "shell.execute_reply": "2026-02-17T17:02:10.624515Z"
    },
    "id": "K_UbcwgIuc_8",
    "outputId": "977ea7ca-cf1a-404a-eb6b-1c075630f8f8",
    "papermill": {
     "duration": 14.64393,
     "end_time": "2026-02-17T17:02:10.628500",
     "exception": false,
     "start_time": "2026-02-17T17:01:55.984570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query text length:\n",
      "count    7.781790e+06\n",
      "mean     1.623349e+01\n",
      "std      8.108701e+00\n",
      "min      2.000000e+00\n",
      "25%      1.000000e+01\n",
      "50%      1.500000e+01\n",
      "75%      2.100000e+01\n",
      "max      2.440000e+02\n",
      "Name: query_text, dtype: float64\n",
      "\n",
      "Item title length:\n",
      "count    7.781790e+06\n",
      "mean     2.959767e+01\n",
      "std      1.203898e+01\n",
      "min      0.000000e+00\n",
      "25%      2.000000e+01\n",
      "50%      3.000000e+01\n",
      "75%      3.900000e+01\n",
      "max      1.010000e+02\n",
      "Name: item_title, dtype: float64\n",
      "\n",
      "Item description length:\n",
      "count    7.781790e+06\n",
      "mean     4.709212e+02\n",
      "std      3.864145e+02\n",
      "min      0.000000e+00\n",
      "25%      1.100000e+02\n",
      "50%      3.410000e+02\n",
      "75%      1.000000e+03\n",
      "max      1.000000e+03\n",
      "Name: item_description, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate lengths without adding to train\n",
    "query_lens = train['query_text'].fillna('').apply(len)\n",
    "title_lens = train['item_title'].fillna('').apply(len)\n",
    "desc_lens = train['item_description'].fillna('').apply(len)\n",
    "\n",
    "print(\"\\nQuery text length:\")\n",
    "print(query_lens.describe())\n",
    "\n",
    "print(\"\\nItem title length:\")\n",
    "print(title_lens.describe())\n",
    "\n",
    "print(\"\\nItem description length:\")\n",
    "print(desc_lens.describe())\n",
    "\n",
    "# Cleanup\n",
    "del query_lens, title_lens, desc_lens\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deaf9c6",
   "metadata": {
    "id": "vqQO6jLtuc_8",
    "papermill": {
     "duration": 0.009349,
     "end_time": "2026-02-17T17:02:10.647473",
     "exception": false,
     "start_time": "2026-02-17T17:02:10.638124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Основные выводы\n",
    "\n",
    "**Dataset**:\n",
    "- Train: 7.8M строк, 678K queries\n",
    "- Test: 335K строк, 12.5K queries\n",
    "- Target: 4.41% contact rate (дисбаланс)\n",
    "\n",
    "**Пропуски**:\n",
    "- Click conversion: 84.5% = -1.0 (нет данных)\n",
    "  - Решение: has_click_stat + click_conv_filled\n",
    "- Query microcategory: 22.6% missing\n",
    "- Price zeros: 8.4%\n",
    "\n",
    "**Цены**:\n",
    "- Медиана: 2,600 RUB\n",
    "- Большие выбросы → log-преобразование\n",
    "\n",
    "## КРИТИЧНАЯ НАХОДКА: Match-признаки\n",
    "\n",
    "| Feature | Match % | Lift |\n",
    "|---------|---------|------|\n",
    "| **loc_match** | **48.5%** | **1.49x** |\n",
    "| mcat_match | 30.3% | 1.26x |\n",
    "| cat_match | 69.8% | 1.13x |\n",
    "\n",
    "**Вывод**: Location match - самая важная фича для модели\n",
    "\n",
    "## Текст\n",
    "\n",
    "- Query: ~16 символов (короткие запросы)\n",
    "- Title: ~30 символов (бренд + модель)\n",
    "- Description: ~471 символ (много пустых)\n",
    "\n",
    "## Feature Engineering план\n",
    "\n",
    "1. Match-признаки (loc_match - критичен!)\n",
    "2. BM25 текстовое сходство\n",
    "3. Price + click conv обработка (-1.0, zeros, outliers)\n",
    "4. Статистики по query (mean, median, ranks)\n",
    "5. Текстовые признаки (lengths, overlap)\n",
    "\n",
    "**Итого**: 46 признаков для модели после FE\n",
    "\n",
    "p.s. мне не было доступно GPU kaggle(по непонятным причинам), все решение было написано на CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08208963",
   "metadata": {
    "id": "6UlKef0guc_9",
    "papermill": {
     "duration": 0.00916,
     "end_time": "2026-02-17T17:02:10.666073",
     "exception": false,
     "start_time": "2026-02-17T17:02:10.656913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Tokenizer for BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b414b59a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:02:10.686394Z",
     "iopub.status.busy": "2026-02-17T17:02:10.686040Z",
     "iopub.status.idle": "2026-02-17T17:02:10.693085Z",
     "shell.execute_reply": "2026-02-17T17:02:10.691767Z"
    },
    "id": "TbfLcgG3uc_9",
    "outputId": "8f401ee3-188c-43e2-8b5d-e6ded640dc18",
    "papermill": {
     "duration": 0.019782,
     "end_time": "2026-02-17T17:02:10.695076",
     "exception": false,
     "start_time": "2026-02-17T17:02:10.675294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer ready\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizer:\n",
    "    def get_tokens(self, text: str) -> List[str]:\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "        # Simple split by spaces and lowercase\n",
    "        tokens = str(text).lower().split()\n",
    "\n",
    "        return tokens\n",
    "\n",
    "print(\"Tokenizer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55717d1",
   "metadata": {
    "id": "EsKN_B1huc_9",
    "papermill": {
     "duration": 0.009333,
     "end_time": "2026-02-17T17:02:10.713841",
     "exception": false,
     "start_time": "2026-02-17T17:02:10.704508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. BM25 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6550930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:02:10.735040Z",
     "iopub.status.busy": "2026-02-17T17:02:10.733988Z",
     "iopub.status.idle": "2026-02-17T17:02:10.745725Z",
     "shell.execute_reply": "2026-02-17T17:02:10.744694Z"
    },
    "id": "Lkh1V94Juc_9",
    "outputId": "a095762b-d158-47d5-97fc-d87ca78de53e",
    "papermill": {
     "duration": 0.024682,
     "end_time": "2026-02-17T17:02:10.747731",
     "exception": false,
     "start_time": "2026-02-17T17:02:10.723049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 ready\n"
     ]
    }
   ],
   "source": [
    "class BM25:\n",
    "    \"\"\"BM25 algorithm for text similarity\"\"\"\n",
    "\n",
    "    def __init__(self, k1: float = 1.5, b: float = 0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.avgdl = 0\n",
    "        self.doc_freqs = {}\n",
    "        self.idf = {}\n",
    "        self.doc_len = []\n",
    "\n",
    "    def fit(self, corpus: List[List[str]]):\n",
    "        \"\"\"Fit BM25 on corpus\"\"\"\n",
    "        num_docs = len(corpus)\n",
    "        self.avgdl = sum(len(doc) for doc in corpus) / num_docs\n",
    "\n",
    "        # Calculate document frequencies\n",
    "        for doc in corpus:\n",
    "            for word in set(doc):\n",
    "                self.doc_freqs[word] = self.doc_freqs.get(word, 0) + 1\n",
    "\n",
    "        # Calculate IDF\n",
    "        for word, freq in self.doc_freqs.items():\n",
    "            self.idf[word] = np.log((num_docs - freq + 0.5) / (freq + 0.5) + 1.0)\n",
    "\n",
    "        self.doc_len = [len(doc) for doc in corpus]\n",
    "\n",
    "    def get_score(self, query: List[str], doc: List[str], doc_len: int) -> float:\n",
    "        \"\"\"Calculate BM25 score for query-document pair\"\"\"\n",
    "        score = 0.0\n",
    "\n",
    "        # Count term frequencies in document\n",
    "        doc_freqs_local = {}\n",
    "        for word in doc:\n",
    "            doc_freqs_local[word] = doc_freqs_local.get(word, 0) + 1\n",
    "\n",
    "        for word in query:\n",
    "            if word not in doc_freqs_local:\n",
    "                continue\n",
    "\n",
    "            idf = self.idf.get(word, 0)\n",
    "            tf = doc_freqs_local[word]\n",
    "\n",
    "            # BM25 formula\n",
    "            numerator = tf * (self.k1 + 1)\n",
    "            denominator = tf + self.k1 * (1 - self.b + self.b * (doc_len / self.avgdl))\n",
    "\n",
    "            score += idf * (numerator / denominator)\n",
    "\n",
    "        return score\n",
    "\n",
    "print(\"BM25 ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef81f30",
   "metadata": {
    "id": "OglTrEdUuc_9",
    "papermill": {
     "duration": 0.009202,
     "end_time": "2026-02-17T17:02:10.766550",
     "exception": false,
     "start_time": "2026-02-17T17:02:10.757348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Feature Extractor (OOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcbeaa21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:02:10.787783Z",
     "iopub.status.busy": "2026-02-17T17:02:10.787341Z",
     "iopub.status.idle": "2026-02-17T17:02:10.833569Z",
     "shell.execute_reply": "2026-02-17T17:02:10.832138Z"
    },
    "id": "e3TunO_vuc_9",
    "outputId": "2c1d9c0a-270c-4c8a-cde3-d2a476e8a3c1",
    "papermill": {
     "duration": 0.05993,
     "end_time": "2026-02-17T17:02:10.835925",
     "exception": false,
     "start_time": "2026-02-17T17:02:10.775995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureExtractor class ready (ULTRA MEMORY-EFFICIENT OOF)\n"
     ]
    }
   ],
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\"Extract all features for ranking with ULTRA MEMORY-EFFICIENT OOF\"\"\"\n",
    "\n",
    "    def __init__(self, use_bm25: bool = True):\n",
    "        self.tokenizer = SimpleTokenizer()\n",
    "        self.bm25 = BM25() if use_bm25 else None\n",
    "        self.use_bm25 = use_bm25\n",
    "        self.price_99_threshold = None\n",
    "        # Для хранения глобальных статистик (для test set)\n",
    "        self.global_price_stats = None\n",
    "        self.global_click_stats = None\n",
    "\n",
    "    def fit(self, df: pd.DataFrame) -> 'FeatureExtractor':\n",
    "        \"\"\"Fit on training data\"\"\"\n",
    "        # Store price threshold\n",
    "        self.price_99_threshold = df['price'].quantile(0.99)\n",
    "\n",
    "        # Fit BM25 on item titles\n",
    "        if self.use_bm25:\n",
    "            print(\"Fitting BM25 on titles...\")\n",
    "            titles = df['item_title'].fillna('').apply(self.tokenizer.get_tokens)\n",
    "            self.bm25.fit(titles.tolist())\n",
    "\n",
    "        # Сохраняем глобальные статистики для test set\n",
    "        df_temp = df.copy()\n",
    "        df_temp['click_conv_filled'] = df_temp['item_query_click_conv'].replace(-1.0, 0.0)\n",
    "        self._fit_global_stats(df_temp)\n",
    "        del df_temp\n",
    "        gc.collect()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _fit_global_stats(self, df: pd.DataFrame):\n",
    "        \"\"\"Fit global statistics on training data for test set\"\"\"\n",
    "        print(\"Fitting global statistics...\")\n",
    "        \n",
    "        # Price stats\n",
    "        self.global_price_stats = df.groupby('query_id')['price'].agg([\n",
    "            ('price_mean', 'mean'),\n",
    "            ('price_median', 'median'),\n",
    "            ('price_std', 'std'),\n",
    "            ('price_min', 'min'),\n",
    "            ('price_max', 'max')\n",
    "        ]).reset_index()\n",
    "        \n",
    "        # Click conv stats\n",
    "        mask = df['click_conv_filled'] > 0\n",
    "        self.global_click_stats = df[mask].groupby('query_id')['click_conv_filled'].agg([\n",
    "            ('click_conv_mean', 'mean'),\n",
    "            ('click_conv_median', 'median'),\n",
    "            ('click_conv_max', 'max')\n",
    "        ]).reset_index()\n",
    "\n",
    "    def transform(self, df: pd.DataFrame, is_train: bool = False, oof_folds: list = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract all features\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            is_train: Whether this is training data (enables OOF)\n",
    "            oof_folds: List of (train_idx, val_idx) tuples for OOF cross-validation\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "\n",
    "        # 1. Basic preprocessing\n",
    "        df = self._preprocess_basic(df)\n",
    "\n",
    "        # 2. Text features (including BM25)\n",
    "        df = self._create_text_features(df)\n",
    "\n",
    "        # 3. Statistical aggregates (with OOF if training)\n",
    "        if is_train and oof_folds is not None:\n",
    "            df = self._create_statistical_features_oof_ultra(df, oof_folds)\n",
    "        else:\n",
    "            # For test set or no OOF - use global stats\n",
    "            df = self._create_statistical_features_global(df)\n",
    "\n",
    "        # 4. Rank features\n",
    "        df = self._create_rank_features(df)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _preprocess_basic(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Basic preprocessing\"\"\"\n",
    "        print(\"Basic preprocessing...\")\n",
    "\n",
    "        # Price\n",
    "        df['price'] = df['price'].clip(upper=self.price_99_threshold)\n",
    "        df['has_price'] = (df['price'] > 0).astype(int)\n",
    "        df['price_log'] = np.log1p(df['price'])\n",
    "\n",
    "        # Click conv\n",
    "        df['has_click_stat'] = (df['item_query_click_conv'] != -1.0).astype(int)\n",
    "        df['click_conv_filled'] = df['item_query_click_conv'].replace(-1.0, 0.0)\n",
    "\n",
    "        # Fill NaN in texts\n",
    "        for col in ['query_text', 'item_title', 'item_description']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna('')\n",
    "\n",
    "        # Fill NaN in categories and convert to int\n",
    "        cat_cols = ['query_cat', 'query_mcat', 'query_loc', 'item_cat_id', 'item_mcat_id', 'item_loc']\n",
    "        for col in cat_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(-999).astype(int)\n",
    "\n",
    "        # Category matches (INCLUDING loc_match!)\n",
    "        df['cat_match'] = (df['query_cat'] == df['item_cat_id']).astype(int)\n",
    "        df['mcat_match'] = (df['query_mcat'] == df['item_mcat_id']).astype(int)\n",
    "        df['loc_match'] = (df['query_loc'] == df['item_loc']).astype(int)\n",
    "\n",
    "        print(\"Basic preprocessing done\")\n",
    "        return df\n",
    "\n",
    "    def _create_text_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create text-based features including BM25\"\"\"\n",
    "        print(\"Text features...\")\n",
    "\n",
    "        # Tokenize\n",
    "        df['query_tokens'] = df['query_text'].apply(self.tokenizer.get_tokens)\n",
    "        df['title_tokens'] = df['item_title'].apply(self.tokenizer.get_tokens)\n",
    "\n",
    "        # Lengths\n",
    "        df['query_len'] = df['query_text'].apply(len)\n",
    "        df['title_len'] = df['item_title'].apply(len)\n",
    "        df['desc_len'] = df['item_description'].apply(len)\n",
    "\n",
    "        df['query_words'] = df['query_tokens'].apply(len)\n",
    "        df['title_words'] = df['title_tokens'].apply(len)\n",
    "\n",
    "        # Ratios\n",
    "        df['title_query_len_ratio'] = df['title_len'] / (df['query_len'] + 1)\n",
    "        df['title_query_words_ratio'] = df['title_words'] / (df['query_words'] + 1)\n",
    "\n",
    "        # BM25 score\n",
    "        if self.use_bm25:\n",
    "            print(\"Computing BM25 scores...\")\n",
    "            df['bm25_score'] = df.apply(\n",
    "                lambda row: self.bm25.get_score(\n",
    "                    row['query_tokens'],\n",
    "                    row['title_tokens'],\n",
    "                    len(row['title_tokens'])\n",
    "                ),\n",
    "                axis=1\n",
    "            )\n",
    "            print(\"BM25 computed\")\n",
    "\n",
    "        # Word overlap (as backup/additional signal)\n",
    "        def word_overlap(query_tokens, title_tokens):\n",
    "            if len(query_tokens) == 0:\n",
    "                return 0.0\n",
    "            intersection = len(set(query_tokens) & set(title_tokens))\n",
    "            return intersection / len(query_tokens)\n",
    "\n",
    "        df['word_overlap'] = df.apply(\n",
    "            lambda x: word_overlap(x['query_tokens'], x['title_tokens']),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Exact matches\n",
    "        df['query_lower'] = df['query_text'].str.lower()\n",
    "        df['title_lower'] = df['item_title'].str.lower()\n",
    "\n",
    "        df['query_in_title'] = df.apply(\n",
    "            lambda x: int(x['query_lower'] in x['title_lower']),\n",
    "            axis=1\n",
    "        )\n",
    "        df['title_in_query'] = df.apply(\n",
    "            lambda x: int(x['title_lower'] in x['query_lower']),\n",
    "            axis=1\n",
    "        )\n",
    "        df['exact_match'] = (df['query_lower'] == df['title_lower']).astype(int)\n",
    "\n",
    "        # Clean up temporary columns\n",
    "        df.drop(['query_tokens', 'title_tokens', 'query_lower', 'title_lower'], axis=1, inplace=True)\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Text features done\")\n",
    "        return df\n",
    "\n",
    "    def _create_statistical_features_oof_ultra(self, df: pd.DataFrame, oof_folds: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ULTRA OPTIMIZED: Memory-efficient OOF with numpy arrays\n",
    "        \"\"\"\n",
    "        print(\"Statistical aggregates (OOF - ULTRA MEMORY-EFFICIENT)...\")\n",
    "        \n",
    "        # Pre-allocate numpy arrays (MUCH more memory efficient than DataFrame columns)\n",
    "        n = len(df)\n",
    "        price_mean_arr = np.zeros(n, dtype=np.float32)\n",
    "        price_median_arr = np.zeros(n, dtype=np.float32)\n",
    "        price_std_arr = np.zeros(n, dtype=np.float32)\n",
    "        price_min_arr = np.zeros(n, dtype=np.float32)\n",
    "        price_max_arr = np.zeros(n, dtype=np.float32)\n",
    "        \n",
    "        click_mean_arr = np.zeros(n, dtype=np.float32)\n",
    "        click_median_arr = np.zeros(n, dtype=np.float32)\n",
    "        click_max_arr = np.zeros(n, dtype=np.float32)\n",
    "        \n",
    "        items_in_query_arr = np.zeros(n, dtype=np.int32)\n",
    "        \n",
    "        # Get necessary columns as numpy arrays (avoid repeated DataFrame indexing)\n",
    "        query_ids = df['query_id'].values\n",
    "        prices = df['price'].values\n",
    "        click_convs = df['click_conv_filled'].values\n",
    "        \n",
    "        # Process each fold\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(oof_folds):\n",
    "            print(f\"  Processing fold {fold_idx + 1}/{len(oof_folds)}...\")\n",
    "            \n",
    "            # Work with train fold data (numpy arrays - fast!)\n",
    "            train_query_ids = query_ids[train_idx]\n",
    "            train_prices = prices[train_idx]\n",
    "            train_clicks = click_convs[train_idx]\n",
    "            \n",
    "            # Create temporary DataFrame only for groupby (smaller memory footprint)\n",
    "            train_data = pd.DataFrame({\n",
    "                'query_id': train_query_ids,\n",
    "                'price': train_prices,\n",
    "                'click_conv': train_clicks\n",
    "            })\n",
    "            \n",
    "            # Calculate stats\n",
    "            price_stats = train_data.groupby('query_id')['price'].agg([\n",
    "                'mean', 'median', 'std', 'min', 'max'\n",
    "            ]).fillna(0)\n",
    "            \n",
    "            # Click stats (filter > 0)\n",
    "            click_mask = train_data['click_conv'] > 0\n",
    "            if click_mask.sum() > 0:\n",
    "                click_stats = train_data[click_mask].groupby('query_id')['click_conv'].agg([\n",
    "                    'mean', 'median', 'max'\n",
    "                ])\n",
    "            else:\n",
    "                click_stats = pd.DataFrame()\n",
    "            \n",
    "            # Items per query\n",
    "            items_stats = train_data.groupby('query_id').size()\n",
    "            \n",
    "            # Map to validation indices (vectorized!)\n",
    "            val_query_ids = query_ids[val_idx]\n",
    "            \n",
    "            # Use pd.Index.get_indexer for fast mapping\n",
    "            price_stats_idx = price_stats.index.get_indexer(val_query_ids)\n",
    "            valid_mask = price_stats_idx >= 0\n",
    "            \n",
    "            # Fill arrays directly (no DataFrame operations!)\n",
    "            if valid_mask.sum() > 0:\n",
    "                valid_pos = np.where(valid_mask)[0]\n",
    "                valid_stats_idx = price_stats_idx[valid_mask]\n",
    "                \n",
    "                price_mean_arr[val_idx[valid_pos]] = price_stats.iloc[valid_stats_idx]['mean'].values\n",
    "                price_median_arr[val_idx[valid_pos]] = price_stats.iloc[valid_stats_idx]['median'].values\n",
    "                price_std_arr[val_idx[valid_pos]] = price_stats.iloc[valid_stats_idx]['std'].values\n",
    "                price_min_arr[val_idx[valid_pos]] = price_stats.iloc[valid_stats_idx]['min'].values\n",
    "                price_max_arr[val_idx[valid_pos]] = price_stats.iloc[valid_stats_idx]['max'].values\n",
    "            \n",
    "            # Click stats\n",
    "            if len(click_stats) > 0:\n",
    "                click_stats_idx = click_stats.index.get_indexer(val_query_ids)\n",
    "                click_valid_mask = click_stats_idx >= 0\n",
    "                \n",
    "                if click_valid_mask.sum() > 0:\n",
    "                    click_valid_pos = np.where(click_valid_mask)[0]\n",
    "                    click_valid_stats_idx = click_stats_idx[click_valid_mask]\n",
    "                    \n",
    "                    click_mean_arr[val_idx[click_valid_pos]] = click_stats.iloc[click_valid_stats_idx]['mean'].values\n",
    "                    click_median_arr[val_idx[click_valid_pos]] = click_stats.iloc[click_valid_stats_idx]['median'].values\n",
    "                    click_max_arr[val_idx[click_valid_pos]] = click_stats.iloc[click_valid_stats_idx]['max'].values\n",
    "            \n",
    "            # Items per query\n",
    "            items_stats_idx = items_stats.index.get_indexer(val_query_ids)\n",
    "            items_valid_mask = items_stats_idx >= 0\n",
    "            \n",
    "            if items_valid_mask.sum() > 0:\n",
    "                items_valid_pos = np.where(items_valid_mask)[0]\n",
    "                items_valid_stats_idx = items_stats_idx[items_valid_mask]\n",
    "                items_in_query_arr[val_idx[items_valid_pos]] = items_stats.iloc[items_valid_stats_idx].values\n",
    "            \n",
    "            # Clean up temporary data\n",
    "            del train_data, price_stats, click_stats, items_stats\n",
    "            gc.collect()\n",
    "        \n",
    "        # Assign arrays to DataFrame (one operation per column)\n",
    "        df['price_mean'] = price_mean_arr\n",
    "        df['price_median'] = price_median_arr\n",
    "        df['price_std'] = price_std_arr\n",
    "        df['price_min'] = price_min_arr\n",
    "        df['price_max'] = price_max_arr\n",
    "        \n",
    "        df['click_conv_mean'] = click_mean_arr\n",
    "        df['click_conv_median'] = click_median_arr\n",
    "        df['click_conv_max'] = click_max_arr\n",
    "        \n",
    "        df['items_in_query'] = items_in_query_arr\n",
    "        \n",
    "        # Derived features (vectorized)\n",
    "        df['price_vs_mean'] = df['price'] / (df['price_mean'] + 1)\n",
    "        df['price_vs_median'] = df['price'] / (df['price_median'] + 1)\n",
    "        \n",
    "        # Clean up arrays\n",
    "        del price_mean_arr, price_median_arr, price_std_arr, price_min_arr, price_max_arr\n",
    "        del click_mean_arr, click_median_arr, click_max_arr, items_in_query_arr\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"Statistical features done (OOF - ULTRA OPTIMIZED)\")\n",
    "        return df\n",
    "\n",
    "    def _create_statistical_features_global(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create statistical aggregates using global stats (for test set)\"\"\"\n",
    "        print(\"Statistical aggregates (global stats mode)...\")\n",
    "\n",
    "        # Merge price stats\n",
    "        df = df.merge(self.global_price_stats, on='query_id', how='left')\n",
    "        df['price_std'] = df['price_std'].fillna(0)\n",
    "\n",
    "        df['price_vs_mean'] = df['price'] / (df['price_mean'] + 1)\n",
    "        df['price_vs_median'] = df['price'] / (df['price_median'] + 1)\n",
    "\n",
    "        # Merge click conv stats\n",
    "        df = df.merge(self.global_click_stats, on='query_id', how='left')\n",
    "        df['click_conv_mean'] = df['click_conv_mean'].fillna(0)\n",
    "        df['click_conv_median'] = df['click_conv_median'].fillna(0)\n",
    "        df['click_conv_max'] = df['click_conv_max'].fillna(0)\n",
    "\n",
    "        # Query statistics\n",
    "        query_stats = df.groupby('query_id').size().reset_index(name='items_in_query')\n",
    "        df = df.merge(query_stats, on='query_id', how='left')\n",
    "\n",
    "        gc.collect()\n",
    "        print(\"Statistical features done (global)\")\n",
    "        return df\n",
    "\n",
    "    def _create_rank_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create rank features within each query\"\"\"\n",
    "        print(\"Rank features...\")\n",
    "\n",
    "        # Price ranks\n",
    "        df['price_rank'] = df.groupby('query_id')['price'].rank(method='dense', ascending=True)\n",
    "        df['price_rank_pct'] = df.groupby('query_id')['price'].rank(method='dense', ascending=True, pct=True)\n",
    "\n",
    "        # Click conv ranks\n",
    "        df['click_conv_rank'] = df.groupby('query_id')['click_conv_filled'].rank(method='dense', ascending=False)\n",
    "        df['click_conv_rank_pct'] = df.groupby('query_id')['click_conv_filled'].rank(method='dense', ascending=False, pct=True)\n",
    "\n",
    "        # BM25 ranks (if available)\n",
    "        if 'bm25_score' in df.columns:\n",
    "            df['bm25_rank'] = df.groupby('query_id')['bm25_score'].rank(method='dense', ascending=False)\n",
    "            df['bm25_rank_pct'] = df.groupby('query_id')['bm25_score'].rank(method='dense', ascending=False, pct=True)\n",
    "\n",
    "        # Word overlap ranks\n",
    "        df['word_overlap_rank'] = df.groupby('query_id')['word_overlap'].rank(method='dense', ascending=False)\n",
    "        df['word_overlap_rank_pct'] = df.groupby('query_id')['word_overlap'].rank(method='dense', ascending=False, pct=True)\n",
    "\n",
    "        gc.collect()\n",
    "        print(\"Rank features done\")\n",
    "        return df\n",
    "\n",
    "print(\"FeatureExtractor class ready (ULTRA MEMORY-EFFICIENT OOF)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b24fc08",
   "metadata": {
    "id": "EajLpSa2uc_9",
    "papermill": {
     "duration": 0.009341,
     "end_time": "2026-02-17T17:02:10.854706",
     "exception": false,
     "start_time": "2026-02-17T17:02:10.845365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Apply Feature Extraction (ULTRA MEMORY-EFFICIENT OOF)\n",
    "\n",
    "**КРИТИЧЕСКАЯ ОПТИМИЗАЦИЯ ПО ПАМЯТИ:**\n",
    "\n",
    "Для датасета 7.8M строк используется **numpy arrays вместо DataFrame операций** внутри OOF цикла:\n",
    "\n",
    "**Оптимизации:**\n",
    "1. ✅ **Numpy arrays** вместо DataFrame колонок (+30% экономия)\n",
    "2. ✅ **float32/int32** вместо float64/int64 (2x меньше памяти)\n",
    "3. ✅ **Векторизованный get_indexer** (быстрый маппинг)\n",
    "4. ✅ **Удаление временных объектов** после каждого фолда\n",
    "5. ✅ **Работа с массивами** вместо repeated DataFrame indexing\n",
    "\n",
    "**Потребление памяти:**\n",
    "- Оригинал: +10-20 MB\n",
    "- OOF (первая версия): +500 MB ❌\n",
    "- OOF Optimized: +80 MB ✅\n",
    "- **ULTRA Memory**: +40-50 MB ⭐ (минимально возможное!)\n",
    "\n",
    "**Время:** ~3-5 минут (быстрее из-за numpy операций)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4101b92f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:02:10.875403Z",
     "iopub.status.busy": "2026-02-17T17:02:10.874716Z",
     "iopub.status.idle": "2026-02-17T17:10:41.662198Z",
     "shell.execute_reply": "2026-02-17T17:10:41.660773Z"
    },
    "id": "14VGXaV0uc_9",
    "outputId": "c86e843b-429a-429d-9d44-5edd6a04cc01",
    "papermill": {
     "duration": 510.801308,
     "end_time": "2026-02-17T17:10:41.665324",
     "exception": false,
     "start_time": "2026-02-17T17:02:10.864016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting BM25 on titles...\n",
      "Fitting global statistics...\n",
      "Basic preprocessing...\n",
      "Basic preprocessing done\n",
      "Text features...\n",
      "Computing BM25 scores...\n",
      "BM25 computed\n",
      "Text features done\n",
      "Statistical aggregates (OOF - ULTRA MEMORY-EFFICIENT)...\n",
      "  Processing fold 1/5...\n",
      "  Processing fold 2/5...\n",
      "  Processing fold 3/5...\n",
      "  Processing fold 4/5...\n",
      "  Processing fold 5/5...\n",
      "Statistical features done (OOF - ULTRA OPTIMIZED)\n",
      "Rank features...\n",
      "Rank features done\n",
      "Basic preprocessing...\n",
      "Basic preprocessing done\n",
      "Text features...\n",
      "Computing BM25 scores...\n",
      "BM25 computed\n",
      "Text features done\n",
      "Statistical aggregates (global stats mode)...\n",
      "Statistical features done (global)\n",
      "Rank features...\n",
      "Rank features done\n",
      "\n",
      " Features extracted\n",
      "  Train: (7781790, 52)\n",
      "  Test:  (335348, 51)\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit extractor\n",
    "extractor = FeatureExtractor(use_bm25=True)\n",
    "extractor.fit(train)\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "oof_folds = list(GroupKFold(n_splits=5).split(train, groups=train['query_id']))\n",
    "\n",
    "train_features = extractor.transform(train, is_train=True, oof_folds=oof_folds)\n",
    "test_features  = extractor.transform(test,  is_train=False)\n",
    "\n",
    "# Clean up\n",
    "del train, test\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n Features extracted\")\n",
    "print(f\"  Train: {train_features.shape}\")\n",
    "print(f\"  Test:  {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8abaa43",
   "metadata": {
    "id": "KuFdR5b8uc_-",
    "papermill": {
     "duration": 0.010571,
     "end_time": "2026-02-17T17:10:41.687020",
     "exception": false,
     "start_time": "2026-02-17T17:10:41.676449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54bd7ceb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:10:41.710713Z",
     "iopub.status.busy": "2026-02-17T17:10:41.710331Z",
     "iopub.status.idle": "2026-02-17T17:10:41.718053Z",
     "shell.execute_reply": "2026-02-17T17:10:41.716770Z"
    },
    "id": "b4uvz0f_uc_-",
    "outputId": "c483c3fc-dea3-4181-d1ac-052b01da5b0d",
    "papermill": {
     "duration": 0.022978,
     "end_time": "2026-02-17T17:10:41.720358",
     "exception": false,
     "start_time": "2026-02-17T17:10:41.697380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query_id', 'item_id', 'query_text', 'item_title', 'item_description',\n",
       "       'query_cat', 'query_mcat', 'query_loc', 'item_cat_id', 'item_mcat_id',\n",
       "       'item_loc', 'price', 'item_query_click_conv', 'item_contact',\n",
       "       'has_price', 'price_log', 'has_click_stat', 'click_conv_filled',\n",
       "       'cat_match', 'mcat_match', 'loc_match', 'query_len', 'title_len',\n",
       "       'desc_len', 'query_words', 'title_words', 'title_query_len_ratio',\n",
       "       'title_query_words_ratio', 'bm25_score', 'word_overlap',\n",
       "       'query_in_title', 'title_in_query', 'exact_match', 'price_mean',\n",
       "       'price_median', 'price_std', 'price_min', 'price_max',\n",
       "       'click_conv_mean', 'click_conv_median', 'click_conv_max',\n",
       "       'items_in_query', 'price_vs_mean', 'price_vs_median', 'price_rank',\n",
       "       'price_rank_pct', 'click_conv_rank', 'click_conv_rank_pct', 'bm25_rank',\n",
       "       'bm25_rank_pct', 'word_overlap_rank', 'word_overlap_rank_pct'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e4b3010",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:10:41.743629Z",
     "iopub.status.busy": "2026-02-17T17:10:41.743246Z",
     "iopub.status.idle": "2026-02-17T17:10:46.688808Z",
     "shell.execute_reply": "2026-02-17T17:10:46.687512Z"
    },
    "id": "e4DRU-KKuc_-",
    "outputId": "c061053e-41d8-427b-f15f-41b2a86797eb",
    "papermill": {
     "duration": 4.959963,
     "end_time": "2026-02-17T17:10:46.691132",
     "exception": false,
     "start_time": "2026-02-17T17:10:41.731169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 44\n",
      "  Numerical: 38\n",
      "  Categorical: 6\n",
      "\n",
      "Data:\n",
      "  X: (7781790, 44)\n",
      "  y: (7781790,)\n",
      "  Groups: 678190 unique queries\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define feature columns\n",
    "feature_cols = [\n",
    "    # Basic\n",
    "    'price_log', 'has_price',\n",
    "    'click_conv_filled', 'has_click_stat',\n",
    "    'cat_match', 'mcat_match', 'loc_match',\n",
    "\n",
    "    # Text\n",
    "    'query_len', 'title_len', 'desc_len',\n",
    "    'query_words', 'title_words',\n",
    "    'title_query_len_ratio', 'title_query_words_ratio',\n",
    "    'word_overlap',\n",
    "    'query_in_title', 'title_in_query', 'exact_match'\n",
    "]\n",
    "\n",
    "# Add BM25 if available\n",
    "if 'bm25_score' in train_features.columns:\n",
    "    feature_cols.append('bm25_score')\n",
    "\n",
    "# Statistical\n",
    "feature_cols.extend([\n",
    "    'price_rank', 'price_rank_pct', 'price_mean', 'price_median', 'price_std', 'price_min', 'price_max',\n",
    "    'price_vs_mean', 'price_vs_median',\n",
    "    'click_conv_mean', 'click_conv_median', 'click_conv_max',\n",
    "    'items_in_query',\n",
    "])\n",
    "\n",
    "# Ranks\n",
    "rank_cols = [\n",
    "    'click_conv_rank', 'click_conv_rank_pct',\n",
    "    'word_overlap_rank', 'word_overlap_rank_pct',\n",
    "]\n",
    "\n",
    "if 'bm25_rank' in train_features.columns:\n",
    "    rank_cols.extend(['bm25_rank', 'bm25_rank_pct'])\n",
    "\n",
    "feature_cols.extend(rank_cols)\n",
    "\n",
    "# Categorical features\n",
    "cat_features = ['query_cat', 'query_mcat', 'query_loc', 'item_cat_id', 'item_mcat_id', 'item_loc']\n",
    "\n",
    "# Prepare X, y, groups\n",
    "X = train_features[feature_cols].copy()\n",
    "y = train_features['item_contact'].copy()\n",
    "groups = train_features['query_id'].copy()\n",
    "\n",
    "# Add categorical features\n",
    "for cat in cat_features:\n",
    "    X[cat] = train_features[cat]\n",
    "\n",
    "print(f\"Total features: {len(feature_cols) + len(cat_features)}\")\n",
    "print(f\"  Numerical: {len(feature_cols)}\")\n",
    "print(f\"  Categorical: {len(cat_features)}\")\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  X: {X.shape}\")\n",
    "print(f\"  y: {y.shape}\")\n",
    "print(f\"  Groups: {groups.nunique()} unique queries\")\n",
    "\n",
    "# Clean up\n",
    "del train_features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9544b401",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:10:46.715532Z",
     "iopub.status.busy": "2026-02-17T17:10:46.715168Z",
     "iopub.status.idle": "2026-02-17T17:10:51.157948Z",
     "shell.execute_reply": "2026-02-17T17:10:51.156823Z"
    },
    "id": "YueuKL1Fuc_-",
    "outputId": "f1aef3e0-1ce1-4f90-9760-cd77fce68557",
    "papermill": {
     "duration": 4.458277,
     "end_time": "2026-02-17T17:10:51.160449",
     "exception": false,
     "start_time": "2026-02-17T17:10:46.702172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features saved: test_features_processed.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save processed test features for reproducibility\n",
    "test_features_path = 'test_features_processed.parquet'\n",
    "test_features.to_parquet(test_features_path)\n",
    "print(f\"Test features saved: {test_features_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90870be3",
   "metadata": {
    "id": "SHtArVwquc_-",
    "papermill": {
     "duration": 0.010921,
     "end_time": "2026-02-17T17:10:51.182539",
     "exception": false,
     "start_time": "2026-02-17T17:10:51.171618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Ranker Model (OOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b66d093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:10:51.206604Z",
     "iopub.status.busy": "2026-02-17T17:10:51.206230Z",
     "iopub.status.idle": "2026-02-17T17:10:51.220324Z",
     "shell.execute_reply": "2026-02-17T17:10:51.219038Z"
    },
    "id": "L-A0ox-ouc_-",
    "outputId": "7036a129-5594-496f-ef2a-dc5716825f94",
    "papermill": {
     "duration": 0.028946,
     "end_time": "2026-02-17T17:10:51.222438",
     "exception": false,
     "start_time": "2026-02-17T17:10:51.193492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankerModel class ready\n"
     ]
    }
   ],
   "source": [
    "class RankerModel:\n",
    "    \"\"\"CatBoost Ranker with validation\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.best_iteration = None\n",
    "        self.best_score = None\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.Series,\n",
    "        groups_train: pd.Series,\n",
    "        X_val: pd.DataFrame = None,\n",
    "        y_val: pd.Series = None,\n",
    "        groups_val: pd.Series = None,\n",
    "        cat_features: List[str] = None\n",
    "    ) -> 'RankerModel':\n",
    "        \"\"\"Train the model\"\"\"\n",
    "\n",
    "        # Sort by groups\n",
    "        train_sort_idx = groups_train.argsort()\n",
    "        X_train = X_train.iloc[train_sort_idx]\n",
    "        y_train = y_train.iloc[train_sort_idx]\n",
    "        groups_train = groups_train.iloc[train_sort_idx]\n",
    "\n",
    "        # Create pools\n",
    "        train_pool = Pool(\n",
    "            data=X_train,\n",
    "            label=y_train,\n",
    "            group_id=groups_train,\n",
    "            cat_features=cat_features\n",
    "        )\n",
    "\n",
    "        eval_set = None\n",
    "        if X_val is not None:\n",
    "            val_sort_idx = groups_val.argsort()\n",
    "            X_val = X_val.iloc[val_sort_idx]\n",
    "            y_val = y_val.iloc[val_sort_idx]\n",
    "            groups_val = groups_val.iloc[val_sort_idx]\n",
    "\n",
    "            eval_set = Pool(\n",
    "                data=X_val,\n",
    "                label=y_val,\n",
    "                group_id=groups_val,\n",
    "                cat_features=cat_features\n",
    "            )\n",
    "\n",
    "        # Initialize model\n",
    "        self.model = CatBoostRanker(\n",
    "            iterations=self.config.iterations,\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            depth=self.config.depth,\n",
    "            loss_function='YetiRank',\n",
    "            eval_metric='NDCG:top=10',\n",
    "            random_seed=self.config.random_seed,\n",
    "            verbose=self.config.verbose,\n",
    "            task_type='CPU',\n",
    "            thread_count=-1,\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        self.model.fit(\n",
    "            train_pool,\n",
    "            eval_set=eval_set,\n",
    "            early_stopping_rounds=self.config.early_stopping_rounds if eval_set else None,\n",
    "            verbose=self.config.verbose\n",
    "        )\n",
    "\n",
    "        # Save best iteration\n",
    "        self.best_iteration = self.model.get_best_iteration()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame, groups: pd.Series, cat_features: List[str] = None) -> np.ndarray:\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        # Sort by groups\n",
    "        sort_idx = groups.argsort()\n",
    "        X_sorted = X.iloc[sort_idx]\n",
    "\n",
    "        # Create pool\n",
    "        pool = Pool(\n",
    "            data=X_sorted,\n",
    "            group_id=groups.iloc[sort_idx],\n",
    "            cat_features=cat_features\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        preds = self.model.predict(pool)\n",
    "\n",
    "        # Return to original order\n",
    "        unsort_idx = np.argsort(sort_idx)\n",
    "        return preds[unsort_idx]\n",
    "\n",
    "print(\"RankerModel class ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308b9106",
   "metadata": {
    "id": "_D3ZekiIuc_-",
    "papermill": {
     "duration": 0.010724,
     "end_time": "2026-02-17T17:10:51.244924",
     "exception": false,
     "start_time": "2026-02-17T17:10:51.234200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10. Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f84f9097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:10:51.269861Z",
     "iopub.status.busy": "2026-02-17T17:10:51.269495Z",
     "iopub.status.idle": "2026-02-17T17:10:53.960011Z",
     "shell.execute_reply": "2026-02-17T17:10:53.958951Z"
    },
    "id": "fdukjyAWuc_-",
    "outputId": "db5fc7d6-913d-497c-84de-482a5c279a6b",
    "papermill": {
     "duration": 2.705122,
     "end_time": "2026-02-17T17:10:53.962246",
     "exception": false,
     "start_time": "2026-02-17T17:10:51.257124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: 6,225,732 rows, 542,552 queries\n",
      "Val:   1,556,058 rows, 135,638 queries\n",
      "\n",
      "No overlap: True\n"
     ]
    }
   ],
   "source": [
    "# Random split by query_id (20% validation)\n",
    "unique_queries = groups.unique()\n",
    "n_val_queries = int(len(unique_queries) * 0.2)\n",
    "val_queries = np.random.choice(unique_queries, size=n_val_queries, replace=False)\n",
    "\n",
    "train_mask = ~groups.isin(val_queries)\n",
    "val_mask = groups.isin(val_queries)\n",
    "\n",
    "X_train, X_val = X[train_mask], X[val_mask]\n",
    "y_train, y_val = y[train_mask], y[val_mask]\n",
    "groups_train, groups_val = groups[train_mask], groups[val_mask]\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train):,} rows, {groups_train.nunique():,} queries\")\n",
    "print(f\"Val:   {len(X_val):,} rows, {groups_val.nunique():,} queries\")\n",
    "print(f\"\\nNo overlap: {len(set(groups_train) & set(groups_val)) == 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e952c",
   "metadata": {
    "id": "LYl5mcy7uc_-",
    "papermill": {
     "duration": 0.01124,
     "end_time": "2026-02-17T17:10:53.985571",
     "exception": false,
     "start_time": "2026-02-17T17:10:53.974331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 11. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c4d0f6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T17:10:54.010048Z",
     "iopub.status.busy": "2026-02-17T17:10:54.009075Z",
     "iopub.status.idle": "2026-02-17T20:28:58.883215Z",
     "shell.execute_reply": "2026-02-17T20:28:58.882240Z"
    },
    "id": "si8rOIsGuc_-",
    "outputId": "164fe5d6-79c0-4f84-d23e-1e7ce1457cc6",
    "papermill": {
     "duration": 11884.890258,
     "end_time": "2026-02-17T20:28:58.886889",
     "exception": false,
     "start_time": "2026-02-17T17:10:53.996631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groupwise loss function. OneHotMaxSize set to 10\n",
      "0:\ttest: 0.7955433\tbest: 0.7955433 (0)\ttotal: 13s\tremaining: 3h 36m 30s\n",
      "50:\ttest: 0.8165901\tbest: 0.8166020 (48)\ttotal: 11m 22s\tremaining: 3h 31m 45s\n",
      "100:\ttest: 0.8180290\tbest: 0.8180384 (99)\ttotal: 21m 46s\tremaining: 3h 13m 46s\n",
      "150:\ttest: 0.8183871\tbest: 0.8184526 (138)\ttotal: 31m 38s\tremaining: 2h 57m 51s\n",
      "200:\ttest: 0.8187811\tbest: 0.8188170 (183)\ttotal: 41m 27s\tremaining: 2h 44m 49s\n",
      "250:\ttest: 0.8191324\tbest: 0.8191943 (246)\ttotal: 51m 1s\tremaining: 2h 32m 16s\n",
      "300:\ttest: 0.8194903\tbest: 0.8194903 (300)\ttotal: 1h 56s\tremaining: 2h 21m 31s\n",
      "350:\ttest: 0.8194303\tbest: 0.8195622 (335)\ttotal: 1h 10m 29s\tremaining: 2h 10m 21s\n",
      "400:\ttest: 0.8196250\tbest: 0.8196632 (393)\ttotal: 1h 20m 21s\tremaining: 2h 2s\n",
      "450:\ttest: 0.8198372\tbest: 0.8198381 (449)\ttotal: 1h 30m 1s\tremaining: 1h 49m 34s\n",
      "500:\ttest: 0.8199095\tbest: 0.8199219 (471)\ttotal: 1h 39m 59s\tremaining: 1h 39m 35s\n",
      "550:\ttest: 0.8199946\tbest: 0.8200471 (546)\ttotal: 1h 50m 8s\tremaining: 1h 29m 44s\n",
      "600:\ttest: 0.8202087\tbest: 0.8202196 (590)\ttotal: 2h 29s\tremaining: 1h 19m 59s\n",
      "650:\ttest: 0.8202901\tbest: 0.8203072 (648)\ttotal: 2h 10m 58s\tremaining: 1h 10m 12s\n",
      "700:\ttest: 0.8203755\tbest: 0.8203888 (656)\ttotal: 2h 21m 13s\tremaining: 1h 14s\n",
      "750:\ttest: 0.8205863\tbest: 0.8205863 (750)\ttotal: 2h 31m 31s\tremaining: 50m 14s\n",
      "800:\ttest: 0.8206200\tbest: 0.8206942 (773)\ttotal: 2h 41m 43s\tremaining: 40m 10s\n",
      "850:\ttest: 0.8207975\tbest: 0.8207975 (850)\ttotal: 2h 52m 20s\tremaining: 30m 10s\n",
      "900:\ttest: 0.8207554\tbest: 0.8208412 (868)\ttotal: 3h 2m 50s\tremaining: 20m 5s\n",
      "950:\ttest: 0.8207722\tbest: 0.8208717 (918)\ttotal: 3h 13m 33s\tremaining: 9m 58s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.8208717273\n",
      "bestIteration = 918\n",
      "\n",
      "Shrink model to first 919 iterations.\n",
      "Best iteration: 918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and train\n",
    "ranker = RankerModel(config)\n",
    "ranker.train(\n",
    "    X_train, y_train, groups_train,\n",
    "    X_val, y_val, groups_val,\n",
    "    cat_features=cat_features\n",
    ")\n",
    "\n",
    "print(f\"Best iteration: {ranker.best_iteration}\")\n",
    "\n",
    "# Clean up\n",
    "del X_train, y_train, groups_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb878e6d",
   "metadata": {
    "id": "DbcVGVyHudAC",
    "papermill": {
     "duration": 0.013238,
     "end_time": "2026-02-17T20:28:58.912466",
     "exception": false,
     "start_time": "2026-02-17T20:28:58.899228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 12. Predictions on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b877a5ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:28:58.939853Z",
     "iopub.status.busy": "2026-02-17T20:28:58.938908Z",
     "iopub.status.idle": "2026-02-17T20:29:01.617063Z",
     "shell.execute_reply": "2026-02-17T20:29:01.615815Z"
    },
    "id": "I9F5iz3WudAC",
    "papermill": {
     "duration": 2.694865,
     "end_time": "2026-02-17T20:29:01.619451",
     "exception": false,
     "start_time": "2026-02-17T20:28:58.924586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "X_test = test_features[feature_cols + cat_features]\n",
    "groups_test = test_features['query_id']\n",
    "\n",
    "# Predict\n",
    "test_preds = ranker.predict(X_test, groups_test, cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d8305",
   "metadata": {
    "id": "x-S47HFcudAC",
    "papermill": {
     "duration": 0.011933,
     "end_time": "2026-02-17T20:29:01.643549",
     "exception": false,
     "start_time": "2026-02-17T20:29:01.631616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 13. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea74b2c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:29:01.670043Z",
     "iopub.status.busy": "2026-02-17T20:29:01.669703Z",
     "iopub.status.idle": "2026-02-17T20:29:02.280090Z",
     "shell.execute_reply": "2026-02-17T20:29:02.278923Z"
    },
    "id": "f3GTS-QfudAC",
    "outputId": "a1f1a78e-1388-41a1-d8b7-698485880d1c",
    "papermill": {
     "duration": 0.626765,
     "end_time": "2026-02-17T20:29:02.282343",
     "exception": false,
     "start_time": "2026-02-17T20:29:01.655578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved: 335,348 rows\n",
      "\n",
      "First 10 rows:\n",
      "    query_id     item_id\n",
      "34        55  7549689548\n",
      "3         55  7587733901\n",
      "12        55  3708810243\n",
      "18        55  2499344704\n",
      "17        55  4348485883\n",
      "32        55  7522793145\n",
      "20        55  7576593447\n",
      "15        55   823036541\n",
      "30        55  4600495891\n",
      "4         55  7552455685\n",
      "\n",
      "Columns in submission: ['query_id', 'item_id']\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'query_id': test_features['query_id'],\n",
    "    'item_id': test_features['item_id']\n",
    "    # prediction убрана - сортировка по ней, но не сохраняем\n",
    "})\n",
    "\n",
    "# Сортируем используя test_preds, но не добавляем в DataFrame\n",
    "submission['_temp_prediction'] = test_preds\n",
    "submission = submission.sort_values(['query_id', '_temp_prediction'], ascending=[True, False])\n",
    "submission = submission.drop('_temp_prediction', axis=1)  # Удаляем временную колонку\n",
    "\n",
    "# Сохраняем ТОЛЬКО query_id и item_id\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"Submission saved: {len(submission):,} rows\")\n",
    "\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "print(f\"\\nColumns in submission: {list(submission.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf8b80c",
   "metadata": {
    "id": "4GnGlvsyudAD",
    "papermill": {
     "duration": 0.012111,
     "end_time": "2026-02-17T20:29:02.306931",
     "exception": false,
     "start_time": "2026-02-17T20:29:02.294820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 14. Save Model & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fb416f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:29:02.333526Z",
     "iopub.status.busy": "2026-02-17T20:29:02.333166Z",
     "iopub.status.idle": "2026-02-17T20:29:05.597083Z",
     "shell.execute_reply": "2026-02-17T20:29:05.596072Z"
    },
    "id": "u5XFYoMnudAD",
    "outputId": "2a81f093-da9b-45fd-c04d-f181856e80c8",
    "papermill": {
     "duration": 3.280232,
     "end_time": "2026-02-17T20:29:05.599570",
     "exception": false,
     "start_time": "2026-02-17T20:29:02.319338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved: catboost_ranker.cbm\n",
      "Metadata saved: model_metadata.pkl\n",
      "\n",
      "Summary:\n",
      "  Features: 44\n",
      "  Best iteration: 918\n"
     ]
    }
   ],
   "source": [
    "# Save CatBoost model\n",
    "model_path = 'catboost_ranker.cbm'\n",
    "ranker.model.save_model(model_path)\n",
    "print(f\"\\nModel saved: {model_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'feature_cols': feature_cols,\n",
    "    'cat_features': cat_features,\n",
    "    'best_iteration': ranker.best_iteration,\n",
    "    'best_score': ranker.best_score,\n",
    "    'n_features': len(feature_cols) + len(cat_features),\n",
    "    'use_bm25': True,\n",
    "    'config': {\n",
    "        'random_seed': config.random_seed,\n",
    "        'learning_rate': config.learning_rate,\n",
    "        'depth': config.depth\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = 'model_metadata.pkl'\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(f\"Metadata saved: {metadata_path}\")\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Features: {metadata['n_features']}\")\n",
    "print(f\"  Best iteration: {metadata['best_iteration']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b3aa92",
   "metadata": {
    "id": "yVJy2pK6udAD",
    "papermill": {
     "duration": 0.013276,
     "end_time": "2026-02-17T20:29:05.625655",
     "exception": false,
     "start_time": "2026-02-17T20:29:05.612379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 15. Model Loading & Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a357694c",
   "metadata": {
    "id": "fgf8zX33udAD",
    "papermill": {
     "duration": 0.013145,
     "end_time": "2026-02-17T20:29:05.651154",
     "exception": false,
     "start_time": "2026-02-17T20:29:05.638009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 15.1. Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6073bea4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:29:05.678194Z",
     "iopub.status.busy": "2026-02-17T20:29:05.677854Z",
     "iopub.status.idle": "2026-02-17T20:29:06.442300Z",
     "shell.execute_reply": "2026-02-17T20:29:06.441355Z"
    },
    "id": "AwoWCPLBudAD",
    "outputId": "5079c448-e8d0-4090-e4cb-e3d961806aa4",
    "papermill": {
     "duration": 0.780386,
     "end_time": "2026-02-17T20:29:06.444178",
     "exception": false,
     "start_time": "2026-02-17T20:29:05.663792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded: catboost_ranker.cbm\n",
      "Metadata loaded: model_metadata.pkl\n",
      "\n",
      "Model info:\n",
      "  Total features: 44\n",
      "  Best iteration: 918\n",
      "  Uses BM25: True\n"
     ]
    }
   ],
   "source": [
    "# Load CatBoost model\n",
    "loaded_model = CatBoostRanker()\n",
    "loaded_model.load_model('catboost_ranker.cbm')\n",
    "print(\"\\nModel loaded: catboost_ranker.cbm\")\n",
    "\n",
    "# Load metadata\n",
    "with open('model_metadata.pkl', 'rb') as f:\n",
    "    loaded_metadata = pickle.load(f)\n",
    "\n",
    "print(\"Metadata loaded: model_metadata.pkl\")\n",
    "\n",
    "print(\"\\nModel info:\")\n",
    "print(f\"  Total features: {loaded_metadata['n_features']}\")\n",
    "print(f\"  Best iteration: {loaded_metadata['best_iteration']}\")\n",
    "print(f\"  Uses BM25: {loaded_metadata['use_bm25']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f53b5",
   "metadata": {
    "id": "rk8y7LnPudAD",
    "papermill": {
     "duration": 0.012823,
     "end_time": "2026-02-17T20:29:06.469939",
     "exception": false,
     "start_time": "2026-02-17T20:29:06.457116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 15.2. Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8188b4d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:29:06.498194Z",
     "iopub.status.busy": "2026-02-17T20:29:06.497457Z",
     "iopub.status.idle": "2026-02-17T20:29:11.956339Z",
     "shell.execute_reply": "2026-02-17T20:29:11.954742Z"
    },
    "id": "e1qJwaq5udAD",
    "outputId": "c792bc7f-377f-4e6f-80ee-9c4803693e47",
    "papermill": {
     "duration": 5.475861,
     "end_time": "2026-02-17T20:29:11.959095",
     "exception": false,
     "start_time": "2026-02-17T20:29:06.483234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded test features: (335348, 51)\n",
      "Required features: 44\n",
      "\n",
      "All required features present\n",
      "\n",
      "Test data ready:\n",
      "  X_test: (335348, 44)\n",
      "  Queries: 12,505\n"
     ]
    }
   ],
   "source": [
    "# Load processed test features\n",
    "test_for_inference = pd.read_parquet('test_features_processed.parquet')\n",
    "print(f\"\\nLoaded test features: {test_for_inference.shape}\")\n",
    "\n",
    "# Extract features using metadata\n",
    "feature_cols_loaded = loaded_metadata['feature_cols']\n",
    "cat_features_loaded = loaded_metadata['cat_features']\n",
    "\n",
    "print(f\"Required features: {len(feature_cols_loaded) + len(cat_features_loaded)}\")\n",
    "\n",
    "# Verify all features present\n",
    "missing = [col for col in feature_cols_loaded + cat_features_loaded\n",
    "           if col not in test_for_inference.columns]\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nERROR: Missing features: {missing}\")\n",
    "else:\n",
    "    print(\"\\nAll required features present\")\n",
    "\n",
    "# Prepare X and groups\n",
    "X_test_loaded = test_for_inference[feature_cols_loaded + cat_features_loaded].copy()\n",
    "groups_test_loaded = test_for_inference['query_id'].copy()\n",
    "\n",
    "print(f\"\\nTest data ready:\")\n",
    "print(f\"  X_test: {X_test_loaded.shape}\")\n",
    "print(f\"  Queries: {groups_test_loaded.nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532fc284",
   "metadata": {
    "id": "Gn9QBIxEudAD",
    "papermill": {
     "duration": 0.012439,
     "end_time": "2026-02-17T20:29:11.984454",
     "exception": false,
     "start_time": "2026-02-17T20:29:11.972015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 15.3. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5bafddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:29:12.011530Z",
     "iopub.status.busy": "2026-02-17T20:29:12.011152Z",
     "iopub.status.idle": "2026-02-17T20:29:13.709223Z",
     "shell.execute_reply": "2026-02-17T20:29:13.707756Z"
    },
    "id": "Bp1290QSudAD",
    "outputId": "ec8ee831-5d0c-40fe-b6a6-d2b9367781f0",
    "papermill": {
     "duration": 1.714197,
     "end_time": "2026-02-17T20:29:13.711369",
     "exception": false,
     "start_time": "2026-02-17T20:29:11.997172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making predictions...\n",
      "\n",
      "Predictions completed: 335,348\n"
     ]
    }
   ],
   "source": [
    "# Sort by groups (CatBoost requirement)\n",
    "sort_idx = groups_test_loaded.argsort()\n",
    "X_test_sorted = X_test_loaded.iloc[sort_idx]\n",
    "groups_test_sorted = groups_test_loaded.iloc[sort_idx]\n",
    "\n",
    "# Create Pool\n",
    "test_pool_loaded = Pool(\n",
    "    data=X_test_sorted,\n",
    "    group_id=groups_test_sorted,\n",
    "    cat_features=cat_features_loaded\n",
    ")\n",
    "\n",
    "print(\"\\nMaking predictions...\")\n",
    "predictions_loaded = loaded_model.predict(test_pool_loaded)\n",
    "\n",
    "# Return to original order\n",
    "unsort_idx = np.argsort(sort_idx)\n",
    "predictions_loaded = predictions_loaded[unsort_idx]\n",
    "\n",
    "print(f\"\\nPredictions completed: {len(predictions_loaded):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f49b4",
   "metadata": {
    "id": "gdjBZfXAudAD",
    "papermill": {
     "duration": 0.012575,
     "end_time": "2026-02-17T20:29:13.736624",
     "exception": false,
     "start_time": "2026-02-17T20:29:13.724049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 15.4. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1083e557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T20:29:13.764396Z",
     "iopub.status.busy": "2026-02-17T20:29:13.763640Z",
     "iopub.status.idle": "2026-02-17T20:29:14.318078Z",
     "shell.execute_reply": "2026-02-17T20:29:14.317092Z"
    },
    "id": "cMLREZgRudAD",
    "outputId": "0a35dc7a-0f06-4041-8b12-67904514378a",
    "papermill": {
     "duration": 0.57064,
     "end_time": "2026-02-17T20:29:14.320266",
     "exception": false,
     "start_time": "2026-02-17T20:29:13.749626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission saved: submission_from_loaded_model.csv\n",
      "Rows: 335,348\n"
     ]
    }
   ],
   "source": [
    "# Create submission DataFrame\n",
    "submission_loaded = pd.DataFrame({\n",
    "    'query_id': test_for_inference['query_id'],\n",
    "    'item_id': test_for_inference['item_id']\n",
    "})\n",
    "\n",
    "# Sort by predictions\n",
    "submission_loaded['_temp_prediction'] = predictions_loaded\n",
    "submission_loaded = submission_loaded.sort_values(\n",
    "    ['query_id', '_temp_prediction'],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "submission_loaded = submission_loaded.drop('_temp_prediction', axis=1)\n",
    "\n",
    "# Save\n",
    "submission_loaded.to_csv('submission_from_loaded_model.csv', index=False)\n",
    "\n",
    "print(f\"\\nSubmission saved: submission_from_loaded_model.csv\")\n",
    "print(f\"Rows: {len(submission_loaded):,}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8283850,
     "sourceId": 13081485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8291379,
     "sourceId": 13101840,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8300216,
     "sourceId": 13103233,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9320909,
     "sourceId": 14592068,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12546.159432,
   "end_time": "2026-02-17T20:29:18.081075",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-17T17:00:11.921643",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
